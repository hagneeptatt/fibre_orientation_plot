{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orientation Analysis Plot (Polar coordinates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import math\n",
    "\n",
    "import re \n",
    "\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "\n",
    "import scipy\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.stats import circmean, circstd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pycircstat2\n",
    "from pycircstat2.hypothesis import rao_spacing_test\n",
    "\n",
    "\n",
    "plt.rcParams['svg.fonttype'] = 'none'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing w137 d14 400 con t6_OJ_Distribution...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mbgm4fs3\\AppData\\Local\\Temp\\ipykernel_19524\\4029887444.py:79: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_df = pd.concat([results_df, pd.DataFrame({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing w137 d14 800 con t6_OJ_Distribution...\n",
      "Processing w137 d28 400 con tray 9_OJ_Distribution...\n",
      "Processing w137 d28 400 exp tray 5_OJ_Distribution...\n",
      "Processing w137 d28 800 con tray 5_OJ_Distribution...\n",
      "Processing w137 d28 800 exp tray 4_OJ_Distribution...\n",
      "Processing w164 d14 400 con t5_OJ_Distribution...\n",
      "Processing w164 d14 800 con t6_OJ_Distribution...\n",
      "Processing w164 d28 400 con tray 5_OJ_Distribution...\n",
      "Processing w164 d28 400 exp tray 5_OJ_Distribution...\n",
      "Processing w164 d28 800 con tray 6_OJ_Distribution...\n",
      "Processing w164 d28 800 exp tray 5_OJ_Distribution...\n",
      "Processing w184 d14 400 con t7_OJ_Distribution...\n",
      "Processing w184 d14 800 con t7_OJ_Distribution...\n",
      "Processing w184 d28 400 con tray 7_OJ_Distribution...\n",
      "Processing w184 d28 400 exp tray 4_OJ_Distribution...\n",
      "Processing w184 d28 800 con t8_OJ_Distribution...\n",
      "Processing w184 d28 800 exp tray 5_OJ_Distribution...\n",
      "Processing w184 d7 400 con t5_OJ_Distribution...\n",
      "Processing w184 d7 800 con t5_OJ_Distribution...\n",
      "Analysis complete!\n",
      "Total samples analyzed: 20\n"
     ]
    }
   ],
   "source": [
    "# Set matplotlib parameters for better visualization\n",
    "mpl.rcParams['font.size'] = 25\n",
    "mpl.rcParams['axes.labelsize'] = 25\n",
    "mpl.rcParams['xtick.labelsize'] = 20\n",
    "mpl.rcParams['ytick.labelsize'] = 20\n",
    "mpl.rcParams['legend.fontsize'] = 20\n",
    "\n",
    "# Import data from filepath\n",
    "folder_path = Path(r\"C:\\Users\\mbgm4fs3\\OneDrive - The University of Manchester\\PhD\\Experimental\\Data\\5. Mechanical Stimulation\\Primary\\Imaging\\Pic Red\\analysed\")\n",
    "\n",
    "# Create a DataFrame to store the Fourier analysis results\n",
    "results_df = pd.DataFrame(columns=['Filename', 'NumPeaks', 'H2_Amplitude', 'H4_Amplitude', \n",
    "                                  'H4_to_H2_Ratio', 'DominantHarmonic', \n",
    "                                  'FirstHalfMeanAngle', 'FirstHalfNumPeaks'])\n",
    "\n",
    "for csv_file in folder_path.glob(\"*.csv\"):\n",
    "    title = csv_file.stem\n",
    "    print(f\"Processing {title}...\")\n",
    "\n",
    "    # read in csv file\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # Create a second copy shifted from [0, 180] → [180, 360]\n",
    "    df_copy = df.assign(Orientation=df['Orientation'] + 180)     \n",
    "\n",
    "    # append second half of circular data to first to get 360 degree data\n",
    "    df_circular = pd.concat([df, df_copy], ignore_index=True)\n",
    "\n",
    "    # now lets extend the data -180 and +180 to avoid artefacts when smoothing\n",
    "    df_extended = pd.concat([df_circular.assign(Orientation=df_circular['Orientation'] - 360), \n",
    "                             df_circular, \n",
    "                             df_circular.assign(Orientation=df_circular['Orientation'] + 360)], \n",
    "                             ignore_index=True)\n",
    "\n",
    "    # calculate orientation in radians\n",
    "    df_extended['Orientation_rad'] = df_extended['Orientation'] * (math.pi / 180) \n",
    "\n",
    "    # normalise data \n",
    "    Slice1_min = df_extended['Slice1'].min()\n",
    "    Slice1_max = df_extended['Slice1'].max()\n",
    "\n",
    "    df_extended['Slice1_norm'] = (df_extended['Slice1'] - Slice1_min)/(Slice1_max - Slice1_min)\n",
    "\n",
    "    # smooth signal\n",
    "    df_extended['Slice1_smooth'] = savgol_filter(df_extended['Slice1_norm'], 50, 2)\n",
    "\n",
    "    # Trim back to the original 0–360 range\n",
    "    df_trimmed = df_extended[(df_extended['Orientation'] >= 0) & (df_extended['Orientation'] <= 360)].reset_index(drop=True)\n",
    "\n",
    "    # -----------------------------------------\n",
    "    # Fourier Analysis to assess the circular pattern\n",
    "    # -----------------------------------------\n",
    "    \n",
    "    # Resample the signal to have uniform angular spacing for FFT\n",
    "    angles = np.linspace(0, 2*np.pi, 360)\n",
    "    # Interpolate the smoothed data to uniform angular spacing\n",
    "    signal_interp = np.interp(angles, df_trimmed['Orientation_rad'], df_trimmed['Slice1_smooth'])\n",
    "    \n",
    "    # Apply Fourier Transform\n",
    "    fft_result = np.fft.fft(signal_interp)\n",
    "    fft_amplitude = np.abs(fft_result)\n",
    "    \n",
    "    # The 2nd harmonic corresponds to 2 cycles in 360 degrees (index 2 in FFT)\n",
    "    # The 4th harmonic corresponds to 4 cycles in 360 degrees (index 4 in FFT)\n",
    "    h2_amplitude = fft_amplitude[2] / len(angles)  # Normalize by signal length\n",
    "    h4_amplitude = fft_amplitude[4] / len(angles)\n",
    "    \n",
    "    # Calculate ratio of 4th to 2nd harmonic\n",
    "    h4_to_h2_ratio = h4_amplitude / h2_amplitude if h2_amplitude > 0 else float('inf')\n",
    "    \n",
    "    # Determine dominant harmonic\n",
    "    dominant_harmonic = 4 if h4_amplitude > h2_amplitude else 2\n",
    "\n",
    "    # -----------------------------------------\n",
    "    # Store Results\n",
    "    # -----------------------------------------\n",
    "    \n",
    "    # Store results\n",
    "    results_df = pd.concat([results_df, pd.DataFrame({\n",
    "        'Filename': [title],\n",
    "        'NumPeaks': [num_peaks],\n",
    "        'H2_Amplitude': [h2_amplitude],\n",
    "        'H4_Amplitude': [h4_amplitude],\n",
    "        'H4_to_H2_Ratio': [h4_to_h2_ratio],\n",
    "        'DominantHarmonic': [dominant_harmonic],\n",
    "        'OrientationAnglesMean': [first_half_mean_angle],\n",
    "        'OrientaionPeaks': [first_half_num_peaks],\n",
    "    })], ignore_index=True)\n",
    "\n",
    "    # -----------------------------------------\n",
    "    # Visualization\n",
    "    # -----------------------------------------\n",
    "\n",
    "    # re-bin data to plot rose plot \n",
    "    bin_size = 5\n",
    "\n",
    "    # Assign each orientation to a bin\n",
    "    df_trimmed['Angle_bin'] = (df_trimmed['Orientation'] // bin_size) * bin_size\n",
    "\n",
    "    # Group by bin and aggregate (mean or sum, depending on what you want)\n",
    "    rebinned = df_trimmed.groupby('Angle_bin')['Slice1_smooth'].mean().reset_index()\n",
    "\n",
    "    # If you want it in radians for plotting\n",
    "    rebinned['Angle_bin_rad'] = np.deg2rad(rebinned['Angle_bin'])\n",
    "\n",
    "    # Min-max normalization\n",
    "    min_val = rebinned['Slice1_smooth'].min()\n",
    "    max_val = rebinned['Slice1_smooth'].max()\n",
    "\n",
    "    rebinned['Slice1_smooth_norm'] = (rebinned['Slice1_smooth'] - min_val) / (max_val - min_val)\n",
    "\n",
    "    # -----------------------------------------\n",
    "    # Find Peaks in Binned Data and Calculate angles between peaks - ONLY FIRST HALF (0-180°)\n",
    "    # -----------------------------------------\n",
    "    \n",
    "\n",
    "    # find peaks from data\n",
    "    peaks, props = scipy.signal.find_peaks(rebinned['Slice1_smooth_norm'], distance=9, height=0.1)\n",
    "    num_peaks = len(peaks)\n",
    "    \n",
    "    # -----------------------------------------\n",
    "    # Calculate angles between peaks - ONLY FIRST HALF (0-180°)\n",
    "    # -----------------------------------------\n",
    "    peak_angles_deg = rebinned['Angle_bin'].iloc[peaks].values\n",
    "    \n",
    "    # Filter to only include peaks in the first half (0-180°)\n",
    "    first_half_peaks = peak_angles_deg[(peak_angles_deg >= 0) & (peak_angles_deg <= 180)]\n",
    "    first_half_peaks = np.sort(first_half_peaks)\n",
    "    first_half_num_peaks = len(first_half_peaks)\n",
    "    \n",
    "    # Calculate angular differences between consecutive peaks in first half\n",
    "    first_half_angle_diffs = []\n",
    "    if first_half_num_peaks >= 2:\n",
    "        for i in range(first_half_num_peaks-1):\n",
    "            diff = first_half_peaks[i+1] - first_half_peaks[i]\n",
    "            first_half_angle_diffs.append(diff)\n",
    "        \n",
    "        first_half_mean_angle = np.mean(first_half_angle_diffs)\n",
    "    else:\n",
    "        first_half_mean_angle = np.nan\n",
    "\n",
    "\n",
    "    # Create a figure with 2 subplots: polar plot and FFT spectrum\n",
    "    # fig = plt.figure(figsize=(21, 9))\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "    # # adjust relative widths of each subplot\n",
    "    # gs = gridspec.GridSpec(1, 2, width_ratios=[6, 1])  # wide : narrow\n",
    "\n",
    "    \n",
    "    # Polar plot for the circular data\n",
    "    # need to use add_subplot as both plots require different axis (polar and cartisean)\n",
    "    ax1 = fig.add_subplot(gs[0], projection='polar', facecolor='none')\n",
    "\n",
    "    ax1.set_facecolor('none')\n",
    "\n",
    "    ax1.bar(rebinned['Angle_bin_rad'],rebinned['Slice1_smooth_norm'], width=0.1, label='Normalised Data', color='teal', \n",
    "            edgecolor='black', linewidth=0.8, alpha=0.8)\n",
    "\n",
    "    # Plot all peaks\n",
    "    ax1.plot(rebinned['Angle_bin_rad'].iloc[peaks],\n",
    "            rebinned['Slice1_smooth_norm'].iloc[peaks],\n",
    "            'ro', label=f'Peaks')\n",
    "\n",
    "    # Draw dashed line from peak to origin (0,0)\n",
    "        # After identifying all your peaks\n",
    "    for peak_idx in peaks:\n",
    "        peak_theta = rebinned['Angle_bin_rad'].iloc[peak_idx]\n",
    "        peak_r = rebinned['Slice1_smooth_norm'].iloc[peak_idx]\n",
    "        ax1.plot([peak_theta, peak_theta], [0, peak_r], 'k--', linewidth=1.0) \n",
    "\n",
    "\n",
    "    # After plotting the dashed lines from peaks to origin\n",
    "    # Filter peaks to only include those in the first half (0-180°)\n",
    "    first_half_peaks = [idx for idx in peaks if 0 <= np.degrees(rebinned['Angle_bin_rad'].iloc[idx]) <= 180]\n",
    "    first_half_peak_indices = sorted(first_half_peaks, key=lambda idx: rebinned['Angle_bin_rad'].iloc[idx])\n",
    "\n",
    "    # For each pair of adjacent peaks in the first half\n",
    "    for i in range(len(first_half_peak_indices) - 1):  # Note: -1 to avoid wrapping\n",
    "        # Get current and next peak\n",
    "        current_idx = first_half_peak_indices[i]\n",
    "        next_idx = first_half_peak_indices[i + 1]\n",
    "        \n",
    "        # Get angles for current and next peak\n",
    "        theta1 = rebinned['Angle_bin_rad'].iloc[current_idx]\n",
    "        theta2 = rebinned['Angle_bin_rad'].iloc[next_idx]\n",
    "        \n",
    "        # Calculate radius for arc (use a fraction of the peak r value)\n",
    "        arc_radius = 0.3 * min(rebinned['Slice1_smooth_norm'].iloc[current_idx], \n",
    "                            rebinned['Slice1_smooth_norm'].iloc[next_idx])\n",
    "        \n",
    "        # Generate points for the arc\n",
    "        theta_arc = np.linspace(theta1, theta2, 50)\n",
    "        r_arc = np.ones_like(theta_arc) * arc_radius\n",
    "        \n",
    "        # Plot the arc\n",
    "        ax1.plot(theta_arc, r_arc, 'r-', alpha=0.7)\n",
    "\n",
    "        # Calculate angle in degrees\n",
    "        angle_deg = np.degrees(theta2 - theta1)\n",
    "        \n",
    "        # Add text label for the angle\n",
    "        mid_theta = (theta1 + theta2) / 2\n",
    "        text_radius = arc_radius * 1.7\n",
    "        ax1.text(mid_theta, text_radius, f\"{angle_deg:.1f}°\", \n",
    "                ha='center', va='center', fontsize=10, \n",
    "                bbox=dict(facecolor='white', alpha=0.7, edgecolor='none'))\n",
    "        \n",
    "    \n",
    "    # ax1.set_title(f\"{title}\\nDominant Harmonic: {dominant_harmonic}\")\n",
    "    ax1.legend(loc='lower left', framealpha=1)\n",
    "    # ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    ax1.set_thetamin(0)\n",
    "    ax1.set_thetamax(360)\n",
    "    ax1.set_rmin(0)\n",
    "    ax1.set_rmax(1.1)\n",
    "\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{title}.svg', format='svg', transparent=True, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv('harmonic_analysis_results.csv', index=False)\n",
    "\n",
    "print(\"Analysis complete!\")\n",
    "print(f\"Total samples analyzed: {len(results_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing w137 d14 400 con t6_OJ_Distribution...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mbgm4fs3\\AppData\\Local\\Temp\\ipykernel_19524\\446970735.py:151: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_df = pd.concat([results_df, pd.DataFrame({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing w137 d14 800 con t6_OJ_Distribution...\n",
      "Processing w137 d28 400 con tray 9_OJ_Distribution...\n",
      "Processing w137 d28 400 exp tray 5_OJ_Distribution...\n",
      "Processing w137 d28 800 con tray 5_OJ_Distribution...\n",
      "Processing w137 d28 800 exp tray 4_OJ_Distribution...\n",
      "Processing w164 d14 400 con t5_OJ_Distribution...\n",
      "Processing w164 d14 800 con t6_OJ_Distribution...\n",
      "Processing w164 d28 400 con tray 5_OJ_Distribution...\n",
      "Processing w164 d28 400 exp tray 5_OJ_Distribution...\n",
      "Processing w164 d28 800 con tray 6_OJ_Distribution...\n",
      "Processing w164 d28 800 exp tray 5_OJ_Distribution...\n",
      "Processing w184 d14 400 con t7_OJ_Distribution...\n",
      "Processing w184 d14 800 con t7_OJ_Distribution...\n",
      "Processing w184 d28 400 con tray 7_OJ_Distribution...\n",
      "Processing w184 d28 400 exp tray 4_OJ_Distribution...\n",
      "Processing w184 d28 800 con t8_OJ_Distribution...\n",
      "Processing w184 d28 800 exp tray 5_OJ_Distribution...\n",
      "Processing w184 d7 400 con t5_OJ_Distribution...\n",
      "Processing w184 d7 800 con t5_OJ_Distribution...\n",
      "Analysis complete!\n",
      "Total samples analyzed: 20\n"
     ]
    }
   ],
   "source": [
    "# Set matplotlib parameters for better visualization\n",
    "mpl.rcParams['font.size'] = 25\n",
    "mpl.rcParams['axes.labelsize'] = 25\n",
    "mpl.rcParams['xtick.labelsize'] = 20\n",
    "mpl.rcParams['ytick.labelsize'] = 20\n",
    "mpl.rcParams['legend.fontsize'] = 20\n",
    "\n",
    "# Import data from filepath\n",
    "folder_path = Path(r\"C:\\Users\\mbgm4fs3\\OneDrive - The University of Manchester\\PhD\\Experimental\\Data\\5. Mechanical Stimulation\\Primary\\Imaging\\Pic Red\\analysed\")\n",
    "\n",
    "# Function to find peaks with circular shifts to avoid starting point bias\n",
    "def find_peaks_with_circular_shifts(signal, n_shifts=10, **peak_params):\n",
    "    \"\"\"Find peaks with multiple circular shifts to avoid starting point bias\"\"\"\n",
    "    all_peaks = set()\n",
    "    shift_size = len(signal) // n_shifts\n",
    "    \n",
    "    for i in range(n_shifts):\n",
    "        # Shift the array circularly\n",
    "        shifted_signal = np.roll(signal, i * shift_size)\n",
    "        # Find peaks in shifted array\n",
    "        shift_peaks, _ = scipy.signal.find_peaks(shifted_signal, **peak_params)\n",
    "        # Convert peak indices back to original positions\n",
    "        original_indices = (shift_peaks - i * shift_size) % len(signal)\n",
    "        all_peaks.update(original_indices)\n",
    "    \n",
    "    return sorted(list(all_peaks))\n",
    "\n",
    "# Create a DataFrame to store the Fourier analysis results\n",
    "results_df = pd.DataFrame(columns=['Filename', 'NumPeaks', 'H2_Amplitude', 'H4_Amplitude', \n",
    "                                  'H4_to_H2_Ratio', 'DominantHarmonic', \n",
    "                                  'FirstHalfMeanAngle', 'FirstHalfNumPeaks'])\n",
    "\n",
    "for csv_file in folder_path.glob(\"*.csv\"):\n",
    "    title = csv_file.stem\n",
    "    print(f\"Processing {title}...\")\n",
    "\n",
    "    # read in csv file\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # Create a second copy shifted from [0, 180] → [180, 360]\n",
    "    df_copy = df.assign(Orientation=df['Orientation'] + 180)     \n",
    "\n",
    "    # append second half of circular data to first to get 360 degree data\n",
    "    df_circular = pd.concat([df, df_copy], ignore_index=True)\n",
    "\n",
    "    # now lets extend the data -180 and +180 to avoid artefacts when smoothing\n",
    "    df_extended = pd.concat([df_circular.assign(Orientation=df_circular['Orientation'] - 360), \n",
    "                             df_circular, \n",
    "                             df_circular.assign(Orientation=df_circular['Orientation'] + 360)], \n",
    "                             ignore_index=True)\n",
    "\n",
    "    # calculate orientation in radians\n",
    "    df_extended['Orientation_rad'] = df_extended['Orientation'] * (math.pi / 180) \n",
    "\n",
    "    # normalise data \n",
    "    Slice1_min = df_extended['Slice1'].min()\n",
    "    Slice1_max = df_extended['Slice1'].max()\n",
    "\n",
    "    df_extended['Slice1_norm'] = (df_extended['Slice1'] - Slice1_min)/(Slice1_max - Slice1_min)\n",
    "\n",
    "    # smooth signal\n",
    "    df_extended['Slice1_smooth'] = savgol_filter(df_extended['Slice1_norm'], 50, 2)\n",
    "\n",
    "    # Trim back to the original 0–360 range\n",
    "    df_trimmed = df_extended[(df_extended['Orientation'] >= 0) & (df_extended['Orientation'] <= 360)].reset_index(drop=True)\n",
    "\n",
    "    # -----------------------------------------\n",
    "    # Fourier Analysis to assess the circular pattern\n",
    "    # -----------------------------------------\n",
    "    \n",
    "    # Resample the signal to have uniform angular spacing for FFT\n",
    "    angles = np.linspace(0, 2*np.pi, 360)\n",
    "    # Interpolate the smoothed data to uniform angular spacing\n",
    "    signal_interp = np.interp(angles, df_trimmed['Orientation_rad'], df_trimmed['Slice1_smooth'])\n",
    "    \n",
    "    # Apply Fourier Transform\n",
    "    fft_result = np.fft.fft(signal_interp)\n",
    "    fft_amplitude = np.abs(fft_result)\n",
    "    \n",
    "    # The 2nd harmonic corresponds to 2 cycles in 360 degrees (index 2 in FFT)\n",
    "    # The 4th harmonic corresponds to 4 cycles in 360 degrees (index 4 in FFT)\n",
    "    h2_amplitude = fft_amplitude[2] / len(angles)  # Normalize by signal length\n",
    "    h4_amplitude = fft_amplitude[4] / len(angles)\n",
    "    \n",
    "    # Calculate ratio of 4th to 2nd harmonic\n",
    "    h4_to_h2_ratio = h4_amplitude / h2_amplitude if h2_amplitude > 0 else float('inf')\n",
    "    \n",
    "    # Determine dominant harmonic\n",
    "    dominant_harmonic = 4 if h4_amplitude > h2_amplitude else 2\n",
    "\n",
    "    # -----------------------------------------\n",
    "    # Visualization\n",
    "    # -----------------------------------------\n",
    "\n",
    "    # re-bin data to plot rose plot \n",
    "    bin_size = 5\n",
    "\n",
    "    # Assign each orientation to a bin\n",
    "    df_trimmed['Angle_bin'] = (df_trimmed['Orientation'] // bin_size) * bin_size\n",
    "\n",
    "    # Group by bin and aggregate (mean or sum, depending on what you want)\n",
    "    rebinned = df_trimmed.groupby('Angle_bin')['Slice1_smooth'].mean().reset_index()\n",
    "\n",
    "    # If you want it in radians for plotting\n",
    "    rebinned['Angle_bin_rad'] = np.deg2rad(rebinned['Angle_bin'])\n",
    "\n",
    "    # Min-max normalization\n",
    "    min_val = rebinned['Slice1_smooth'].min()\n",
    "    max_val = rebinned['Slice1_smooth'].max()\n",
    "\n",
    "    rebinned['Slice1_smooth_norm'] = (rebinned['Slice1_smooth'] - min_val) / (max_val - min_val)\n",
    "\n",
    "    # -----------------------------------------\n",
    "    # Find Peaks using the improved circular shifts method\n",
    "    # -----------------------------------------\n",
    "    \n",
    "    # Use our improved peak finding function with circular shifts\n",
    "    peaks = find_peaks_with_circular_shifts(rebinned['Slice1_smooth_norm'].values, \n",
    "                                           n_shifts=10, \n",
    "                                           distance=9, \n",
    "                                           height=0.1)\n",
    "    \n",
    "    num_peaks = len(peaks)\n",
    "    \n",
    "    # -----------------------------------------\n",
    "    # Calculate angles between peaks - ONLY FIRST HALF (0-180°)\n",
    "    # -----------------------------------------\n",
    "    peak_angles_deg = rebinned['Angle_bin'].iloc[peaks].values\n",
    "    \n",
    "    # Filter to only include peaks in the first half (0-180°)\n",
    "    first_half_peaks = peak_angles_deg[(peak_angles_deg >= 0) & (peak_angles_deg <= 180)]\n",
    "    first_half_peaks = np.sort(first_half_peaks)\n",
    "    first_half_num_peaks = len(first_half_peaks)\n",
    "    \n",
    "    # Calculate angular differences between consecutive peaks in first half\n",
    "    first_half_angle_diffs = []\n",
    "    if first_half_num_peaks >= 2:\n",
    "        for i in range(first_half_num_peaks-1):\n",
    "            diff = first_half_peaks[i+1] - first_half_peaks[i]\n",
    "            first_half_angle_diffs.append(diff)\n",
    "        \n",
    "        first_half_mean_angle = np.mean(first_half_angle_diffs)\n",
    "    else:\n",
    "        first_half_mean_angle = np.nan\n",
    "\n",
    "    # -----------------------------------------\n",
    "    # Store Results\n",
    "    # -----------------------------------------\n",
    "    \n",
    "    # Store results\n",
    "    results_df = pd.concat([results_df, pd.DataFrame({\n",
    "        'Filename': [title],\n",
    "        'NumPeaks': [num_peaks],\n",
    "        'H2_Amplitude': [h2_amplitude],\n",
    "        'H4_Amplitude': [h4_amplitude],\n",
    "        'H4_to_H2_Ratio': [h4_to_h2_ratio],\n",
    "        'DominantHarmonic': [dominant_harmonic],\n",
    "        'OrientationAnglesMean': [first_half_mean_angle],\n",
    "        'OrientaionPeaks': [first_half_num_peaks],\n",
    "    })], ignore_index=True)\n",
    "\n",
    "    # Create a figure with transparent background\n",
    "    fig = plt.figure(figsize=(10, 10), facecolor='none')\n",
    "    \n",
    "    # Use just one subplot that takes the entire figure\n",
    "    ax1 = fig.add_subplot(111, projection='polar')\n",
    "    ax1.set_facecolor('none')\n",
    "\n",
    "    # Set width in radians for the bar chart\n",
    "    width_in_radians = np.deg2rad(bin_size)\n",
    "\n",
    "    # Plot the bars\n",
    "    ax1.bar(rebinned['Angle_bin_rad'],\n",
    "            rebinned['Slice1_smooth_norm'], \n",
    "            width=width_in_radians, \n",
    "            label='Normalised Data', \n",
    "            color='teal', \n",
    "            edgecolor='black', \n",
    "            linewidth=0.8, \n",
    "            alpha=0.8)\n",
    "\n",
    "    # Plot all peaks\n",
    "    ax1.plot(rebinned['Angle_bin_rad'].iloc[peaks],\n",
    "            rebinned['Slice1_smooth_norm'].iloc[peaks],\n",
    "            'ro', label=f'Peaks')\n",
    "\n",
    "    # Draw dashed line from peak to origin (0,0)\n",
    "    for peak_idx in peaks:\n",
    "        peak_theta = rebinned['Angle_bin_rad'].iloc[peak_idx]\n",
    "        peak_r = rebinned['Slice1_smooth_norm'].iloc[peak_idx]\n",
    "        ax1.plot([peak_theta, peak_theta], [0, peak_r], 'k--', linewidth=1.0) \n",
    "\n",
    "    # After plotting the dashed lines from peaks to origin\n",
    "    # Filter peaks to only include those in the first half (0-180°)\n",
    "    first_half_peaks = [idx for idx in peaks if 0 <= np.degrees(rebinned['Angle_bin_rad'].iloc[idx]) <= 180]\n",
    "    first_half_peak_indices = sorted(first_half_peaks, key=lambda idx: rebinned['Angle_bin_rad'].iloc[idx])\n",
    "\n",
    "    # For each pair of adjacent peaks in the first half\n",
    "    for i in range(len(first_half_peak_indices) - 1):  # Note: -1 to avoid wrapping\n",
    "        # Get current and next peak\n",
    "        current_idx = first_half_peak_indices[i]\n",
    "        next_idx = first_half_peak_indices[i + 1]\n",
    "        \n",
    "        # Get angles for current and next peak\n",
    "        theta1 = rebinned['Angle_bin_rad'].iloc[current_idx]\n",
    "        theta2 = rebinned['Angle_bin_rad'].iloc[next_idx]\n",
    "        \n",
    "        # Calculate radius for arc (use a fraction of the peak r value)\n",
    "        arc_radius = 0.3 * min(rebinned['Slice1_smooth_norm'].iloc[current_idx], \n",
    "                            rebinned['Slice1_smooth_norm'].iloc[next_idx])\n",
    "        \n",
    "        # Generate points for the arc\n",
    "        theta_arc = np.linspace(theta1, theta2, 50)\n",
    "        r_arc = np.ones_like(theta_arc) * arc_radius\n",
    "        \n",
    "        # Plot the arc\n",
    "        ax1.plot(theta_arc, r_arc, 'r-', alpha=0.7)\n",
    "\n",
    "        # Calculate angle in degrees\n",
    "        angle_deg = np.degrees(theta2 - theta1)\n",
    "        \n",
    "        # Add text label for the angle\n",
    "        mid_theta = (theta1 + theta2) / 2\n",
    "        text_radius = arc_radius * 1.7\n",
    "        ax1.text(mid_theta, text_radius, f\"{angle_deg:.1f}°\", \n",
    "                ha='center', va='center', fontsize=10, \n",
    "                bbox=dict(facecolor='white', alpha=0.7, edgecolor='none'))\n",
    "    \n",
    "    # Format the plot\n",
    "    ax1.legend(loc='lower left', framealpha=1)\n",
    "    ax1.grid(True)\n",
    "    ax1.set_thetamin(0)\n",
    "    ax1.set_thetamax(360)\n",
    "    ax1.set_rmin(0)\n",
    "    ax1.set_rmax(1.1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{title}.svg', format='svg', transparent=True, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv('harmonic_analysis_results.csv', index=False)\n",
    "\n",
    "print(\"Analysis complete!\")\n",
    "print(f\"Total samples analyzed: {len(results_df)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
